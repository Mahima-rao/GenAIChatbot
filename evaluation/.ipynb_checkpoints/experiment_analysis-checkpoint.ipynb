{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We'll generate a new improved version of the evaluation notebook code\n",
    "# based on semantic_eval_results.json and policy_eval_crossencoder.json,\n",
    "# focusing on clearly presenting chatbot performance and insights.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_code = \"\"\"\n",
    "# ü§ñ Chatbot Experiment & Evaluation Notebook\n",
    "\n",
    "This notebook evaluates chatbot performance based on:\n",
    "- ‚úÖ Intent classification and tool invocation\n",
    "- ‚úÖ Policy matching accuracy\n",
    "- ‚úÖ Semantic similarity between expected vs generated responses\n",
    "- ‚úÖ End-to-end reasoning accuracy\n",
    "\n",
    "---\n",
    "\n",
    "## üì• Load Evaluation Results\n",
    "\n",
    "```python\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load semantic evaluation results\n",
    "with open(\"evaluation/semantic_eval_results.json\") as f:\n",
    "    sem = pd.DataFrame(json.load(f))\n",
    "\n",
    "# Load policy-level crossencoder evaluation\n",
    "with open(\"evaluation/policy_eval_semantic.json\") as f:\n",
    "    pol = pd.DataFrame(json.load(f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 1. Semantic Score Distribution (LLM response accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(sem[\"semantic_score\"], bins=10, color=\"lightblue\", edgecolor=\"black\")\n",
    "plt.axvline(0.50, color=\"red\", linestyle=\"--\", label=\"Threshold = 0.50\")\n",
    "plt.title(\"LLM Semantic Score Distribution\")\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Number of Test Cases\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ 2. Overall Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"Intent Accuracy\": sem[\"intent_ok\"].mean(),\n",
    "    \"Tool Accuracy\": sem[\"tool_ok\"].mean(),\n",
    "    \"Response Accuracy\": sem[\"response_ok\"].mean(),\n",
    "    \"Confidence ‚â• 0.50\": sem[\"confidence_ok\"].mean(),\n",
    "    \"End-to-End Accuracy\": sem[\"passed\"].mean()\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(metrics.keys(), [v * 100 for v in metrics.values()], color=\"seagreen\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"End-to-End Evaluation Metrics\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 110)\n",
    "plt.grid(axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà 3. Policy Match Confidence (CrossEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [f\"Test {i+1}\" for i in range(len(pol))]\n",
    "colors = ['green' if r else 'red' for r in pol[\"response_ok\"]]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(labels, pol[\"crossencoder_score\"], color=colors)\n",
    "plt.axhline(0.50, linestyle='--', color='gray', label=\"Semantic Threshold (0.50)\")\n",
    "plt.title(\"Policy Response Semantic Confidence\")\n",
    "plt.xlabel(\"Test Case\")\n",
    "plt.ylabel(\"Cosine Similarity\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ùå 4. Failure Cases Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_cases = sem[~sem[\"passed\"]]\n",
    "failed_cases[[\"message\", \"predicted_intent\", \"predicted_tool\", \"semantic_score\", \"best_matched_phrase\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã 5. Detailed Case Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in sem.iterrows():\n",
    "    print(f\"--- Test {i+1} ---\")\n",
    "    print(f\"Message: {row['message']}\")\n",
    "    print(f\"Predicted Intent ‚Üí {row['predicted_intent']}\")\n",
    "    print(f\"Tool Used        ‚Üí {row['predicted_tool']}\")\n",
    "    print(f\"Matched Phrase   ‚Üí {row['best_matched_phrase']}\")\n",
    "    print(f\"Semantic Score   ‚Üí {row['semantic_score']:.2f}\")\n",
    "    print(f\"Passed           ‚Üí {'‚úÖ' if row['passed'] else '‚ùå'}\\\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Summary & Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance Highlights:**\n",
    "- ‚úÖ High intent classification and tool routing accuracy\n",
    "- ‚úÖ Semantic responses were strong in 5/6 user-level tests\n",
    "- ‚ö†Ô∏è Some LLM replies were semantically weak due to overlong responses or extra explanation\n",
    "\n",
    "**Policy-level Evaluation:**\n",
    "- All 5 policy retrievals matched correctly\n",
    "- One response had semantic drift, dropping crossencoder score < 0.5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
