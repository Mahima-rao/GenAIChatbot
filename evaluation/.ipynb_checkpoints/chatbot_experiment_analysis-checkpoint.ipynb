{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "438382cb", "cell_type": "markdown", "source": "\n# \ud83e\udd16 Chatbot Experiment & Evaluation Notebook\n\nThis notebook evaluates chatbot performance based on:\n- Step-by-step decision-making (intent \u2192 tool \u2192 response)\n- Policy matching accuracy\n- Semantic similarity between expected vs generated responses\n", "metadata": {}}, {"id": "33caf7fb", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nwith open(\"evaluation/semantic_eval_results.json\") as f:\n    sem = pd.DataFrame(json.load(f))\n\nwith open(\"evaluation/policy_eval_semantic.json\") as f:\n    pol = pd.DataFrame(json.load(f))\n", "outputs": []}, {"id": "3e79f5df", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\nplt.figure(figsize=(8, 4))\nplt.hist(sem[\"semantic_score\"], bins=10, color=\"lightblue\", edgecolor=\"black\")\nplt.axvline(0.55, color=\"red\", linestyle=\"--\", label=\"Threshold = 0.55\")\nplt.title(\"LLM Semantic Score Distribution\")\nplt.xlabel(\"Cosine Similarity\")\nplt.ylabel(\"Test Count\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n", "outputs": []}, {"id": "fecf9f2b", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\nmetrics = {\n    \"Intent Accuracy\": sem[\"intent_ok\"].mean(),\n    \"Tool Accuracy\": sem[\"tool_ok\"].mean(),\n    \"Response Accuracy\": sem[\"response_ok\"].mean(),\n    \"Confidence \u2265 0.75\": sem[\"confidence_ok\"].mean(),\n    \"End-to-End Accuracy\": sem[\"passed\"].mean()\n}\n\nplt.figure(figsize=(8, 4))\nplt.bar(metrics.keys(), [v * 100 for v in metrics.values()], color=\"seagreen\")\nplt.ylabel(\"Accuracy (%)\")\nplt.title(\"End-to-End Evaluation Metrics\")\nplt.xticks(rotation=45)\nplt.ylim(0, 110)\nplt.grid(axis=\"y\")\nplt.tight_layout()\nplt.show()\n\nmetrics\n", "outputs": []}, {"id": "50d525f9", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\nlabels = [f\"Test {i+1}\" for i in range(len(pol))]\ncolors = ['green' if r else 'red' for r in pol[\"response_ok\"]]\n\nplt.figure(figsize=(10, 4))\nplt.bar(labels, pol[\"semantic_score\"], color=colors)\nplt.axhline(0.55, linestyle='--', color='gray', label=\"Semantic Threshold (0.55)\")\nplt.title(\"Policy Response Semantic Confidence\")\nplt.xlabel(\"Test Case\")\nplt.ylabel(\"Cosine Similarity\")\nplt.xticks(rotation=45)\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n", "outputs": []}, {"id": "ec3ca196", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\nfailed_cases = sem[~sem[\"passed\"]]\nfailed_cases[[\"message\", \"predicted_intent\", \"predicted_tool\", \"semantic_score\", \"matched_sentence\"]]\n", "outputs": []}, {"id": "2e82c014", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\nfor i, row in sem.iterrows():\n    print(f\"--- Test {i+1} ---\")\n    print(f\"Message: {row['message']}\")\n    print(f\"Predicted Intent \u2192 {row['predicted_intent']}\")\n    print(f\"Tool Used       \u2192 {row['predicted_tool']}\")\n    print(f\"Matched Sentence\u2192 {row['matched_sentence']}\")\n    print(f\"Semantic Score  \u2192 {row['semantic_score']:.2f}\")\n    print(f\"Passed          \u2192 {'\u2705' if row['passed'] else '\u274c'}\\n\")\n", "outputs": []}, {"id": "92d7a104", "cell_type": "markdown", "source": "\n## \ud83d\udd0d Summary & Insights\n\n**Performance:**\n- \u2705 Intent recognition is high\n- \u26a0\ufe0f Tool routing failed in 1 case\n- \u2705 Most LLM replies match expectations semantically\n- \u2705 5 out of 6 tests were successful end-to-end\n\n**Suggestions:**\n- Improve routing logic for tools by using more context\n- Tune or summarize LLM responses to boost semantic score\n- Use multiple reference phrases to improve test coverage\n", "metadata": {}}]}